{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications au Machine Learning et Continuité\n",
    "## Illustrations et Applications Pratiques\n",
    "\n",
    "Ce notebook illustre les applications des concepts mathématiques au machine learning et les propriétés de continuité des fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration pour de meilleurs graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTIE 1 : APPLICATIONS AU MACHINE LEARNING\n",
    "\n",
    "## 1. Espaces de Paramètres et Convergence des Algorithmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir une fonction objectif convexe\n",
    "def fonction_perte(theta, X, y):\n",
    "    \"\"\"Fonction de perte MSE : L(θ) = (1/2n)||Xθ - y||²\"\"\"\n",
    "    predictions = X @ theta\n",
    "    return 0.5 * np.mean((y - predictions)**2)\n",
    "\n",
    "def gradient_perte(theta, X, y):\n",
    "    \"\"\"Gradient : ∇L(θ) = -(1/n)X^T(y - Xθ)\"\"\"\n",
    "    predictions = X @ theta\n",
    "    return -(1/len(y)) * X.T @ (y - predictions)\n",
    "\n",
    "def descente_gradient(X, y, alpha, n_iterations, theta_init=None):\n",
    "    \"\"\"Descente de gradient : θ^(k+1) = θ^(k) - α∇f(θ^(k))\"\"\"\n",
    "    n, p = X.shape\n",
    "    \n",
    "    if theta_init is None:\n",
    "        theta = np.zeros(p)\n",
    "    else:\n",
    "        theta = theta_init.copy()\n",
    "    \n",
    "    historique_theta = [theta.copy()]\n",
    "    historique_perte = [fonction_perte(theta, X, y)]\n",
    "    historique_gradient = [np.linalg.norm(gradient_perte(theta, X, y))]\n",
    "    \n",
    "    for k in range(n_iterations):\n",
    "        grad = gradient_perte(theta, X, y)\n",
    "        theta = theta - alpha * grad\n",
    "        \n",
    "        historique_theta.append(theta.copy())\n",
    "        historique_perte.append(fonction_perte(theta, X, y))\n",
    "        historique_gradient.append(np.linalg.norm(grad))\n",
    "    \n",
    "    return np.array(historique_theta), historique_perte, historique_gradient\n",
    "\n",
    "# Générer des données synthétiques\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 2\n",
    "\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "theta_true = np.array([3.0, -2.0])\n",
    "y = X @ theta_true + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Solution analytique (optimum)\n",
    "theta_star = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "# Exécuter la descente de gradient\n",
    "alpha = 0.1\n",
    "n_iter = 100\n",
    "theta_init = np.array([0.0, 0.0])\n",
    "\n",
    "hist_theta, hist_perte, hist_grad = descente_gradient(X, y, alpha, n_iter, theta_init)\n",
    "\n",
    "print(\"=== Convergence de la Descente de Gradient ===\")\n",
    "print(f\"\\nThéorème : Si f est convexe, différentiable, avec gradients Lipschitz\")\n",
    "print(f\"           et pas α bien choisi, alors θ^(k) → θ* (minimum global)\")\n",
    "print(f\"\\nParamètres vrais    : θ_true = {theta_true}\")\n",
    "print(f\"Optimum théorique   : θ* = {theta_star}\")\n",
    "print(f\"Point initial       : θ^(0) = {theta_init}\")\n",
    "print(f\"Taux d'apprentissage: α = {alpha}\")\n",
    "print(f\"\\nRésultats après {n_iter} itérations :\")\n",
    "print(f\"θ^({n_iter}) = {hist_theta[-1]}\")\n",
    "print(f\"Distance à l'optimum : ||θ^({n_iter}) - θ*|| = {np.linalg.norm(hist_theta[-1] - theta_star):.6f}\")\n",
    "\n",
    "# Critères de convergence\n",
    "epsilon = 1e-4\n",
    "k_conv_param = np.where(np.linalg.norm(np.diff(hist_theta, axis=0), axis=1) < epsilon)[0]\n",
    "k_conv_grad = np.where(np.array(hist_grad) < epsilon)[0]\n",
    "\n",
    "print(f\"\\n=== Critères Pratiques de Convergence ===\")\n",
    "print(f\"\\n1. Critère sur les paramètres : ||θ^(k+1) - θ^(k)|| < ε = {epsilon}\")\n",
    "if len(k_conv_param) > 0:\n",
    "    print(f\"   Convergence atteinte à l'itération k = {k_conv_param[0]}\")\n",
    "else:\n",
    "    print(f\"   Convergence non atteinte avec ce critère\")\n",
    "\n",
    "print(f\"\\n2. Critère sur le gradient : ||∇f(θ^(k))|| < ε = {epsilon}\")\n",
    "if len(k_conv_grad) > 0:\n",
    "    print(f\"   Convergence atteinte à l'itération k = {k_conv_grad[0]}\")\n",
    "else:\n",
    "    print(f\"   Convergence non atteinte avec ce critère\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la convergence\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Graphique 1: Trajectoire dans l'espace des paramètres\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.plot(hist_theta[:, 0], hist_theta[:, 1], 'b-', linewidth=2, marker='o', \n",
    "         markersize=4, alpha=0.6, markevery=5, label='Trajectoire θ^(k)')\n",
    "ax1.scatter(*theta_init, c='green', s=200, marker='o', \n",
    "           edgecolors='black', linewidth=2, label='θ^(0) (initial)', zorder=5)\n",
    "ax1.scatter(*theta_star, c='red', s=300, marker='*', \n",
    "           edgecolors='black', linewidth=2, label='θ* (optimum)', zorder=5)\n",
    "\n",
    "# Contours de la fonction de perte\n",
    "theta1_range = np.linspace(theta_star[0] - 2, theta_star[0] + 2, 100)\n",
    "theta2_range = np.linspace(theta_star[1] - 2, theta_star[1] + 2, 100)\n",
    "T1, T2 = np.meshgrid(theta1_range, theta2_range)\n",
    "Z = np.zeros_like(T1)\n",
    "for i in range(T1.shape[0]):\n",
    "    for j in range(T1.shape[1]):\n",
    "        theta_temp = np.array([T1[i,j], T2[i,j]])\n",
    "        Z[i,j] = fonction_perte(theta_temp, X, y)\n",
    "\n",
    "contours = ax1.contour(T1, T2, Z, levels=20, alpha=0.4, cmap='viridis')\n",
    "ax1.clabel(contours, inline=True, fontsize=8)\n",
    "\n",
    "ax1.set_xlabel('θ₁', fontweight='bold', fontsize=12)\n",
    "ax1.set_ylabel('θ₂', fontweight='bold', fontsize=12)\n",
    "ax1.set_title('Convergence dans l\\'Espace des Paramètres', fontweight='bold', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 2: Décroissance de la fonction de perte\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.semilogy(range(len(hist_perte)), hist_perte, 'b-', linewidth=2)\n",
    "ax2.axhline(y=hist_perte[-1], color='r', linestyle='--', linewidth=1, \n",
    "           label=f'L(θ*) ≈ {hist_perte[-1]:.4f}')\n",
    "ax2.set_xlabel('Itération k', fontweight='bold', fontsize=12)\n",
    "ax2.set_ylabel('L(θ^(k))', fontweight='bold', fontsize=12)\n",
    "ax2.set_title('Décroissance de la Fonction de Perte\\n(échelle log)', fontweight='bold', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 3: Norme du gradient\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.semilogy(range(len(hist_grad)), hist_grad, 'g-', linewidth=2)\n",
    "ax3.axhline(y=epsilon, color='r', linestyle='--', linewidth=2, \n",
    "           label=f'Seuil ε = {epsilon}')\n",
    "ax3.set_xlabel('Itération k', fontweight='bold', fontsize=12)\n",
    "ax3.set_ylabel('||∇L(θ^(k))||', fontweight='bold', fontsize=12)\n",
    "ax3.set_title('Norme du Gradient\\n(critère de convergence)', fontweight='bold', fontsize=14)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/user-data/outputs/convergence_ml.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Régularisation : L1 (LASSO) vs L2 (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer des données avec beaucoup de features (certaines non pertinentes)\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 50\n",
    "n_informative = 10  # Seulement 10 features sont vraiment pertinentes\n",
    "\n",
    "X, y = make_regression(n_samples=n_samples, n_features=n_features, \n",
    "                       n_informative=n_informative, noise=10, random_state=42)\n",
    "\n",
    "# Standardiser les données\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "y = (y - y.mean()) / y.std()\n",
    "\n",
    "# Entraîner différents modèles\n",
    "lambda_val = 0.1\n",
    "\n",
    "# Sans régularisation\n",
    "model_ols = LinearRegression()\n",
    "model_ols.fit(X, y)\n",
    "coef_ols = model_ols.coef_\n",
    "\n",
    "# Régularisation L2 (Ridge): min L(θ) + λ||θ||₂²\n",
    "model_ridge = Ridge(alpha=lambda_val)\n",
    "model_ridge.fit(X, y)\n",
    "coef_ridge = model_ridge.coef_\n",
    "\n",
    "# Régularisation L1 (LASSO): min L(θ) + λ||θ||₁\n",
    "model_lasso = Lasso(alpha=lambda_val)\n",
    "model_lasso.fit(X, y)\n",
    "coef_lasso = model_lasso.coef_\n",
    "\n",
    "print(\"=== Régularisation et Normes ===\")\n",
    "print(f\"\\nProblème d'optimisation régularisé :\")\n",
    "print(f\"  min_θ [L(θ) + λR(θ)]\")\n",
    "print(f\"\\nDonnées : {n_samples} échantillons, {n_features} features (dont {n_informative} pertinentes)\")\n",
    "print(f\"Coefficient de régularisation : λ = {lambda_val}\\n\")\n",
    "\n",
    "print(\"1. SANS RÉGULARISATION (OLS)\")\n",
    "print(f\"   Nombre de coefficients non nuls : {np.sum(np.abs(coef_ols) > 0.01)}\")\n",
    "print(f\"   ||θ||₁ = {np.linalg.norm(coef_ols, ord=1):.4f}\")\n",
    "print(f\"   ||θ||₂ = {np.linalg.norm(coef_ols, ord=2):.4f}\")\n",
    "\n",
    "print(f\"\\n2. RÉGULARISATION L2 - RIDGE : R(θ) = ||θ||₂²\")\n",
    "print(f\"   Favorise des paramètres PETITS, solution STABLE\")\n",
    "print(f\"   Nombre de coefficients non nuls : {np.sum(np.abs(coef_ridge) > 0.01)}\")\n",
    "print(f\"   ||θ||₁ = {np.linalg.norm(coef_ridge, ord=1):.4f}\")\n",
    "print(f\"   ||θ||₂ = {np.linalg.norm(coef_ridge, ord=2):.4f}\")\n",
    "print(f\"   → Tous les coefficients RÉDUITS uniformément\")\n",
    "\n",
    "print(f\"\\n3. RÉGULARISATION L1 - LASSO : R(θ) = ||θ||₁\")\n",
    "print(f\"   Favorise la PARCIMONIE (sélection de features)\")\n",
    "print(f\"   Nombre de coefficients non nuls : {np.sum(np.abs(coef_lasso) > 0.01)}\")\n",
    "print(f\"   ||θ||₁ = {np.linalg.norm(coef_lasso, ord=1):.4f}\")\n",
    "print(f\"   ||θ||₂ = {np.linalg.norm(coef_lasso, ord=2):.4f}\")\n",
    "print(f\"   → SEULEMENT {np.sum(np.abs(coef_lasso) > 0.01)} features sélectionnées!\")\n",
    "\n",
    "# Application 1.8 : Sélection de variables en finance\n",
    "print(\"\\n=== Application 1.8 : Sélection de Variables en Finance ===\")\n",
    "print(\"\\nProblème : Prédire le rendement d'une action à partir de 100 indicateurs\")\n",
    "print(\"\\nAvec régularisation L1 (LASSO) :\")\n",
    "print(\"  → Le modèle sélectionne automatiquement les 5-10 indicateurs les plus pertinents\")\n",
    "print(\"  → Met les autres coefficients à ZÉRO\")\n",
    "print(\"  → Modèle plus INTERPRÉTABLE et moins sujet au SURAPPRENTISSAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des coefficients\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Sans régularisation\n",
    "axes[0].bar(range(n_features), coef_ols, alpha=0.7, edgecolor='black')\n",
    "axes[0].axhline(y=0, color='k', linewidth=0.8)\n",
    "axes[0].set_xlabel('Feature', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('Coefficient', fontweight='bold', fontsize=12)\n",
    "axes[0].set_title(f'Sans Régularisation (OLS)\\n{np.sum(np.abs(coef_ols) > 0.01)} coefficients non nuls',\n",
    "                 fontweight='bold', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Régularisation L2 (Ridge)\n",
    "axes[1].bar(range(n_features), coef_ridge, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[1].axhline(y=0, color='k', linewidth=0.8)\n",
    "axes[1].set_xlabel('Feature', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylabel('Coefficient', fontweight='bold', fontsize=12)\n",
    "axes[1].set_title(f'Régularisation L² (Ridge)\\nλ={lambda_val}\\nTous réduits uniformément',\n",
    "                 fontweight='bold', fontsize=14, color='blue')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Régularisation L1 (LASSO)\n",
    "colors = ['red' if abs(c) > 0.01 else 'lightgray' for c in coef_lasso]\n",
    "axes[2].bar(range(n_features), coef_lasso, alpha=0.7, color=colors, edgecolor='black')\n",
    "axes[2].axhline(y=0, color='k', linewidth=0.8)\n",
    "axes[2].set_xlabel('Feature', fontweight='bold', fontsize=12)\n",
    "axes[2].set_ylabel('Coefficient', fontweight='bold', fontsize=12)\n",
    "axes[2].set_title(f'Régularisation L¹ (LASSO)\\nλ={lambda_val}\\nPARCIMONIE : {np.sum(np.abs(coef_lasso) > 0.01)} features sélectionnées',\n",
    "                 fontweight='bold', fontsize=14, color='red')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/user-data/outputs/regularisation_ml.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distances et Similarité de Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer plusieurs modèles\n",
    "lambdas = [0.01, 0.1, 0.5, 1.0, 5.0]\n",
    "modeles = {}\n",
    "\n",
    "for lam in lambdas:\n",
    "    model = Ridge(alpha=lam)\n",
    "    model.fit(X, y)\n",
    "    modeles[lam] = model.coef_\n",
    "\n",
    "print(\"=== Distances entre Modèles ===\")\n",
    "print(\"\\nDistance entre deux modèles : d(θ₁, θ₂) = ||θ₁ - θ₂||₂\\n\")\n",
    "\n",
    "# Calculer la matrice de distances\n",
    "n_models = len(lambdas)\n",
    "distance_matrix = np.zeros((n_models, n_models))\n",
    "\n",
    "for i, lam1 in enumerate(lambdas):\n",
    "    for j, lam2 in enumerate(lambdas):\n",
    "        distance_matrix[i, j] = np.linalg.norm(modeles[lam1] - modeles[lam2])\n",
    "\n",
    "print(\"Matrice des distances entre modèles Ridge avec différents λ :\")\n",
    "print(f\"\\n{'λ':>8}\", end=\"\")\n",
    "for lam in lambdas:\n",
    "    print(f\"{lam:>8.2f}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * (8 * (n_models + 1)))\n",
    "\n",
    "for i, lam1 in enumerate(lambdas):\n",
    "    print(f\"{lam1:>8.2f}\", end=\"\")\n",
    "    for j in range(n_models):\n",
    "        print(f\"{distance_matrix[i, j]:>8.4f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nObservation : Plus λ est différent, plus les modèles sont éloignés\")\n",
    "\n",
    "# Distance entre prédictions\n",
    "print(\"\\n=== Distance entre Prédictions ===\")\n",
    "print(\"\\nPour évaluer la qualité : Erreur = ||y - ŷ||₂\\n\")\n",
    "\n",
    "for lam in lambdas[:3]:\n",
    "    model = Ridge(alpha=lam)\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    erreur = np.linalg.norm(y - y_pred)\n",
    "    rmse = erreur / np.sqrt(len(y))\n",
    "    print(f\"λ = {lam:5.2f} : Erreur = ||y - ŷ||₂ = {erreur:.4f}, RMSE = {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'évolution des coefficients avec λ\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Graphique 1: Heatmap des distances\n",
    "im = ax1.imshow(distance_matrix, cmap='YlOrRd', aspect='auto')\n",
    "ax1.set_xticks(range(n_models))\n",
    "ax1.set_yticks(range(n_models))\n",
    "ax1.set_xticklabels([f'{lam:.2f}' for lam in lambdas])\n",
    "ax1.set_yticklabels([f'{lam:.2f}' for lam in lambdas])\n",
    "ax1.set_xlabel('λ₂', fontweight='bold', fontsize=12)\n",
    "ax1.set_ylabel('λ₁', fontweight='bold', fontsize=12)\n",
    "ax1.set_title('Distances entre Modèles Ridge\\nd(θ_λ₁, θ_λ₂) = ||θ_λ₁ - θ_λ₂||₂', \n",
    "             fontweight='bold', fontsize=14)\n",
    "\n",
    "# Ajouter les valeurs dans les cellules\n",
    "for i in range(n_models):\n",
    "    for j in range(n_models):\n",
    "        text = ax1.text(j, i, f'{distance_matrix[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax1, label='Distance')\n",
    "\n",
    "# Graphique 2: Évolution des normes avec λ\n",
    "normes_l1 = [np.linalg.norm(modeles[lam], ord=1) for lam in lambdas]\n",
    "normes_l2 = [np.linalg.norm(modeles[lam], ord=2) for lam in lambdas]\n",
    "\n",
    "ax2.plot(lambdas, normes_l1, 'o-', linewidth=2, markersize=8, label='||θ||₁')\n",
    "ax2.plot(lambdas, normes_l2, 's-', linewidth=2, markersize=8, label='||θ||₂')\n",
    "ax2.set_xlabel('λ (coefficient de régularisation)', fontweight='bold', fontsize=12)\n",
    "ax2.set_ylabel('Norme des paramètres', fontweight='bold', fontsize=12)\n",
    "ax2.set_title('Effet de la Régularisation\\nsur la Norme des Paramètres', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/user-data/outputs/distances_modeles.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConclusion : Plus λ augmente, plus les paramètres sont petits (shrinkage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTIE 2 : CONTINUITÉ ET LIMITES\n",
    "\n",
    "## 4. Définition de la Continuité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples de fonctions continues et discontinues\n",
    "\n",
    "def fonction_continue(x):\n",
    "    \"\"\"Fonction polynomiale : f(x) = x² - 2x + 1\"\"\"\n",
    "    return x**2 - 2*x + 1\n",
    "\n",
    "def fonction_heaviside(x):\n",
    "    \"\"\"Fonction de Heaviside (discontinue en 0)\"\"\"\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "def valeur_absolue(x):\n",
    "    \"\"\"Fonction valeur absolue (continue mais non différentiable en 0)\"\"\"\n",
    "    return np.abs(x)\n",
    "\n",
    "print(\"=== Définition de la Continuité ===\")\n",
    "print(\"\\nf est continue en x₀ si :\")\n",
    "print(\"  ∀ε > 0, ∃δ > 0, ∀x : |x - x₀| < δ ⟹ |f(x) - f(x₀)| < ε\")\n",
    "print(\"\\nFormulation séquentielle :\")\n",
    "print(\"  xₙ → x₀ ⟹ f(xₙ) → f(x₀)\")\n",
    "print(\"\\nInterprétation : Petites variations de l'entrée → Petites variations de la sortie\")\n",
    "\n",
    "# Test de continuité\n",
    "x0 = 0\n",
    "epsilon = 0.5\n",
    "\n",
    "print(f\"\\n=== Exemples ===\")\n",
    "print(f\"\\n1. Fonction CONTINUE : f(x) = x² - 2x + 1\")\n",
    "print(f\"   • Polynomiale → continue partout\")\n",
    "print(f\"   • f(0) = {fonction_continue(0)}\")\n",
    "\n",
    "print(f\"\\n2. Fonction DISCONTINUE : H(x) = Heaviside\")\n",
    "print(f\"   • H(x) = 0 si x < 0, H(x) = 1 si x ≥ 0\")\n",
    "print(f\"   • Saut brutal en x = 0\")\n",
    "print(f\"   • lim_(x→0⁻) H(x) = 0 ≠ 1 = H(0)\")\n",
    "\n",
    "print(f\"\\n3. Fonction CONTINUE mais NON DIFFÉRENTIABLE : f(x) = |x|\")\n",
    "print(f\"   • Continue en x = 0 (pas de saut)\")\n",
    "print(f\"   • Mais pas de dérivée en x = 0 (point anguleux)\")\n",
    "\n",
    "# Visualisation\n",
    "x = np.linspace(-3, 3, 1000)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Fonction continue\n",
    "axes[0].plot(x, fonction_continue(x), 'b-', linewidth=2.5)\n",
    "axes[0].scatter([x0], [fonction_continue(x0)], c='red', s=200, zorder=5, \n",
    "               edgecolors='black', linewidth=2)\n",
    "axes[0].set_xlabel('x', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('f(x)', fontweight='bold', fontsize=12)\n",
    "axes[0].set_title('Fonction Continue\\nf(x) = x² - 2x + 1', \n",
    "                 fontweight='bold', fontsize=14, color='green')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Fonction de Heaviside\n",
    "x_heavy = np.linspace(-3, 3, 1000)\n",
    "axes[1].plot(x_heavy[x_heavy < 0], fonction_heaviside(x_heavy[x_heavy < 0]), \n",
    "            'b-', linewidth=2.5)\n",
    "axes[1].plot(x_heavy[x_heavy >= 0], fonction_heaviside(x_heavy[x_heavy >= 0]), \n",
    "            'b-', linewidth=2.5)\n",
    "axes[1].scatter([0], [0], c='blue', s=100, zorder=5, facecolors='none', \n",
    "               edgecolors='blue', linewidth=3)\n",
    "axes[1].scatter([0], [1], c='blue', s=100, zorder=5, edgecolors='black', linewidth=2)\n",
    "axes[1].plot([0, 0], [0, 1], 'r--', linewidth=2, alpha=0.7, label='Discontinuité')\n",
    "axes[1].set_xlabel('x', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylabel('H(x)', fontweight='bold', fontsize=12)\n",
    "axes[1].set_title('Fonction DISCONTINUE\\nH(x) = Heaviside', \n",
    "                 fontweight='bold', fontsize=14, color='red')\n",
    "axes[1].set_ylim([-0.5, 1.5])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Valeur absolue\n",
    "axes[2].plot(x, valeur_absolue(x), 'b-', linewidth=2.5)\n",
    "axes[2].scatter([0], [0], c='red', s=200, zorder=5, edgecolors='black', linewidth=2)\n",
    "axes[2].annotate('Point anguleux\\n(non différentiable)', xy=(0, 0), \n",
    "                xytext=(0.8, 1), fontsize=11, color='red',\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "axes[2].set_xlabel('x', fontweight='bold', fontsize=12)\n",
    "axes[2].set_ylabel('|x|', fontweight='bold', fontsize=12)\n",
    "axes[2].set_title('Continue mais NON Différentiable\\nf(x) = |x|', \n",
    "                 fontweight='bold', fontsize=14, color='orange')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[2].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/user-data/outputs/continuite_fonctions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Continuité des Fonctions de Perte en ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de perte communes en ML\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"Mean Squared Error : L = (1/n)Σ(y - ŷ)²\"\"\"\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def mae_loss(y_true, y_pred):\n",
    "    \"\"\"Mean Absolute Error : L = (1/n)Σ|y - ŷ|\"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def log_loss(y_true, y_pred_prob):\n",
    "    \"\"\"Binary Cross-Entropy : L = -Σ[y log(p) + (1-y)log(1-p)]\"\"\"\n",
    "    epsilon = 1e-15\n",
    "    y_pred_prob = np.clip(y_pred_prob, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred_prob) + (1 - y_true) * np.log(1 - y_pred_prob))\n",
    "\n",
    "print(\"=== Application 1.9 : Continuité des Fonctions de Perte ===\")\n",
    "print(\"\\nLa plupart des fonctions de perte en ML sont CONTINUES\")\n",
    "print(\"\\n1. Mean Squared Error (MSE) :\")\n",
    "print(\"   L(θ) = (1/n)Σ(yᵢ - f_θ(xᵢ))²\")\n",
    "print(\"   → Continue si f_θ est continue en θ\")\n",
    "print(\"\\n2. Cross-Entropy :\")\n",
    "print(\"   L(θ) = -Σyᵢ log(p_θ(xᵢ))\")\n",
    "print(\"   → Continue si p_θ(x) > 0 pour tout x\")\n",
    "print(\"\\nImportance : La continuité garantit qu'on peut optimiser par descente de gradient\")\n",
    "\n",
    "# Générer des données de test\n",
    "y_true_reg = np.array([1, 2, 3, 4, 5])\n",
    "predictions = np.linspace(0, 6, 100)\n",
    "\n",
    "# Calculer les pertes pour différentes prédictions\n",
    "mse_values = [mse_loss(y_true_reg, pred * np.ones_like(y_true_reg)) for pred in predictions]\n",
    "mae_values = [mae_loss(y_true_reg, pred * np.ones_like(y_true_reg)) for pred in predictions]\n",
    "\n",
    "# Pour cross-entropy (classification binaire)\n",
    "y_true_class = np.array([0, 0, 1, 1, 1])\n",
    "probs = np.linspace(0.01, 0.99, 100)\n",
    "ce_values = [log_loss(y_true_class, p * np.ones_like(y_true_class)) for p in probs]\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# MSE\n",
    "axes[0].plot(predictions, mse_values, 'b-', linewidth=2.5)\n",
    "min_idx = np.argmin(mse_values)\n",
    "axes[0].scatter([predictions[min_idx]], [mse_values[min_idx]], \n",
    "               c='red', s=200, zorder=5, edgecolors='black', linewidth=2,\n",
    "               label=f'Minimum en ŷ={predictions[min_idx]:.2f}')\n",
    "axes[0].set_xlabel('Prédiction ŷ', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('MSE Loss', fontweight='bold', fontsize=12)\n",
    "axes[0].set_title('Mean Squared Error\\n(Continue et Convexe)', \n",
    "                 fontweight='bold', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(predictions, mae_values, 'g-', linewidth=2.5)\n",
    "min_idx = np.argmin(mae_values)\n",
    "axes[1].scatter([predictions[min_idx]], [mae_values[min_idx]], \n",
    "               c='red', s=200, zorder=5, edgecolors='black', linewidth=2,\n",
    "               label=f'Minimum en ŷ={predictions[min_idx]:.2f}')\n",
    "axes[1].set_xlabel('Prédiction ŷ', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylabel('MAE Loss', fontweight='bold', fontsize=12)\n",
    "axes[1].set_title('Mean Absolute Error\\n(Continue mais non lisse)', \n",
    "                 fontweight='bold', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Cross-Entropy\n",
    "axes[2].plot(probs, ce_values, 'r-', linewidth=2.5)\n",
    "axes[2].set_xlabel('Probabilité prédite p', fontweight='bold', fontsize=12)\n",
    "axes[2].set_ylabel('Cross-Entropy Loss', fontweight='bold', fontsize=12)\n",
    "axes[2].set_title('Binary Cross-Entropy\\n(Continue pour p ∈ (0,1))', \n",
    "                 fontweight='bold', fontsize=14)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_ylim([0, 3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/user-data/outputs/fonctions_perte.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Théorème des Valeurs Intermédiaires (TVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithme_dichotomie(f, a, b, tolerance=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Algorithme de dichotomie pour trouver un zéro de f dans [a,b]\n",
    "    Suppose que f(a) et f(b) sont de signes opposés\n",
    "    \"\"\"\n",
    "    if f(a) * f(b) > 0:\n",
    "        print(\"Erreur: f(a) et f(b) doivent être de signes opposés\")\n",
    "        return None\n",
    "    \n",
    "    historique = [(a, b, (a+b)/2, f((a+b)/2))]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        c = (a + b) / 2\n",
    "        fc = f(c)\n",
    "        \n",
    "        if abs(fc) < tolerance or (b - a) < tolerance:\n",
    "            return c, historique\n",
    "        \n",
    "        if f(a) * fc < 0:\n",
    "            b = c\n",
    "        else:\n",
    "            a = c\n",
    "        \n",
    "        historique.append((a, b, c, fc))\n",
    "    \n",
    "    return (a + b) / 2, historique\n",
    "\n",
    "# Exemple : Trouver la racine de f(x) = x² - 2 (pour trouver √2)\n",
    "def fonction_test(x):\n",
    "    return x**2 - 2\n",
    "\n",
    "print(\"=== Théorème des Valeurs Intermédiaires ===\")\n",
    "print(\"\\nThéorème (TVI) :\")\n",
    "print(\"  Si f : [a,b] → ℝ est continue et f(a)·f(b) < 0\")\n",
    "print(\"  alors ∃c ∈ (a,b) tel que f(c) = 0\")\n",
    "print(\"\\nInterprétation : Une fonction continue ne peut pas 'sauter'\")\n",
    "print(\"                 par-dessus zéro sans le traverser\")\n",
    "\n",
    "# Application 1.10 : Algorithme de dichotomie\n",
    "print(\"\\n=== Application 1.10 : Algorithme de Dichotomie ===\")\n",
    "print(\"\\nProblème : Trouver √2 en résolvant f(x) = x² - 2 = 0\")\n",
    "\n",
    "a, b = 0, 2\n",
    "tolerance = 1e-6\n",
    "\n",
    "print(f\"\\nIntervalle initial : [{a}, {b}]\")\n",
    "print(f\"f({a}) = {fonction_test(a):.4f}\")\n",
    "print(f\"f({b}) = {fonction_test(b):.4f}\")\n",
    "print(f\"f(a)·f(b) = {fonction_test(a) * fonction_test(b):.4f} < 0 ✓\")\n",
    "\n",
    "racine, historique = algorithme_dichotomie(fonction_test, a, b, tolerance)\n",
    "\n",
    "print(f\"\\nTolérance : {tolerance}\")\n",
    "print(f\"Nombre d'itérations : {len(historique)}\")\n",
    "print(f\"Racine trouvée : x* = {racine:.10f}\")\n",
    "print(f\"√2 (référence) :     {np.sqrt(2):.10f}\")\n",
    "print(f\"Erreur : {abs(racine - np.sqrt(2)):.2e}\")\n",
    "print(f\"\\nComplexité : O(log(1/ε)) = O(log(1/{tolerance})) = {np.log2(1/tolerance):.1f} itérations théoriques\")\n",
    "\n",
    "# Quelques itérations\n",
    "print(\"\\nPremières itérations :\")\n",
    "print(f\"{'k':>3} {'a':>12} {'b':>12} {'c':>12} {'f(c)':>12} {'b-a':>12}\")\n",
    "print(\"-\" * 75)\n",
    "for i, (a_k, b_k, c_k, fc_k) in enumerate(historique[:10]):\n",
    "    print(f\"{i:3d} {a_k:12.8f} {b_k:12.8f} {c_k:12.8f} {fc_k:12.8f} {b_k-a_k:12.8f}\")\n",
    "if len(historique) > 10:\n",
    "    print(\"...\")\n",
    "    i, (a_k, b_k, c_k, fc_k) = len(historique)-1, historique[-1]\n",
    "    print(f\"{i:3d} {a_k:12.8f} {b_k:12.8f} {c_k:12.8f} {fc_k:12.8f} {b_k-a_k:12.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la dichotomie\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Graphique 1: Fonction et intervalles successifs\n",
    "x = np.linspace(-0.5, 2.5, 1000)\n",
    "y = fonction_test(x)\n",
    "\n",
    "ax1.plot(x, y, 'b-', linewidth=2.5, label='f(x) = x² - 2')\n",
    "ax1.axhline(y=0, color='k', linewidth=1)\n",
    "ax1.axvline(x=np.sqrt(2), color='r', linestyle='--', linewidth=2, \n",
    "           label=f'Racine = √2 ≈ {np.sqrt(2):.6f}')\n",
    "\n",
    "# Montrer quelques itérations\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, min(6, len(historique))))\n",
    "for i, (a_k, b_k, c_k, fc_k) in enumerate(historique[:6]):\n",
    "    ax1.plot([a_k, b_k], [0, 0], 'o-', color=colors[i], linewidth=3, \n",
    "            markersize=8, alpha=0.6, label=f'Iter {i}: [{a_k:.3f}, {b_k:.3f}]')\n",
    "\n",
    "ax1.set_xlabel('x', fontweight='bold', fontsize=12)\n",
    "ax1.set_ylabel('f(x)', fontweight='bold', fontsize=12)\n",
    "ax1.set_title('Algorithme de Dichotomie\\nRéduction de l\\'intervalle', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax1.legend(loc='upper left', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([-2.5, 4])\n",
    "\n",
    "# Graphique 2: Convergence\n",
    "erreurs = [abs(c_k - np.sqrt(2)) for (_, _, c_k, _) in historique]\n",
    "largeurs = [b_k - a_k for (a_k, b_k, _, _) in historique]\n",
    "\n",
    "ax2.semilogy(range(len(erreurs)), erreurs, 'ro-', linewidth=2, \n",
    "            markersize=6, label='Erreur |c - √2|')\n",
    "ax2.semilogy(range(len(largeurs)), largeurs, 'bs-', linewidth=2, \n",
    "            markersize=6, label='Largeur intervalle b-a')\n",
    "ax2.axhline(y=tolerance, color='g', linestyle='--', linewidth=2, \n",
    "           label=f'Tolérance = {tolerance}')\n",
    "ax2.set_xlabel('Itération k', fontweight='bold', fontsize=12)\n",
    "ax2.set_ylabel('Erreur / Largeur (échelle log)', fontweight='bold', fontsize=12)\n",
    "ax2.set_title('Convergence Exponentielle\\nde la Dichotomie', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/user-data/outputs/dichotomie.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Application 1.11 : Point fixe\n",
    "print(\"\\n=== Application 1.11 : Point Fixe et Équilibre ===\")\n",
    "print(\"\\nDéfinition : x* est un point fixe de f si f(x*) = x*\")\n",
    "print(\"\\nPour trouver un point fixe, on résout g(x) = f(x) - x = 0\")\n",
    "print(\"Si g est continue avec g(a) < 0 et g(b) > 0,\")\n",
    "print(\"alors ∃ point fixe dans [a,b] (par TVI)\")\n",
    "print(\"\\nApplication financière : Prix d'équilibre où offre = demande\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Théorème des Bornes Atteintes (Weierstrass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Théorème des Bornes Atteintes (Weierstrass) ===\")\n",
    "print(\"\\nThéorème :\")\n",
    "print(\"  Si f : K → ℝ est continue sur un compact K,\")\n",
    "print(\"  alors f est bornée et ATTEINT ses bornes\")\n",
    "print(\"\\nConséquence MAJEURE :\")\n",
    "print(\"  Sur un compact, min f(x) EXISTE TOUJOURS si f est continue\")\n",
    "print(\"                  x∈K\")\n",
    "\n",
    "# Exemple : fonction sur compact vs non-compact\n",
    "def f_compact(x):\n",
    "    \"\"\"Fonction sur compact [0, 2π]\"\"\"\n",
    "    return np.sin(x) + 0.1 * x\n",
    "\n",
    "def f_non_compact(x):\n",
    "    \"\"\"Fonction sur [0, +∞) : inf existe mais n'est pas atteint\"\"\"\n",
    "    return np.exp(-x)\n",
    "\n",
    "# Cas 1: Compact\n",
    "x_compact = np.linspace(0, 2*np.pi, 1000)\n",
    "y_compact = f_compact(x_compact)\n",
    "min_idx = np.argmin(y_compact)\n",
    "max_idx = np.argmax(y_compact)\n",
    "\n",
    "print(\"\\n=== Exemple 1 : Fonction sur Compact [0, 2π] ===\")\n",
    "print(f\"f(x) = sin(x) + 0.1x sur [0, 2π]\")\n",
    "print(f\"\\nMinimum ATTEINT en x = {x_compact[min_idx]:.4f}, f(x) = {y_compact[min_idx]:.4f}\")\n",
    "print(f\"Maximum ATTEINT en x = {x_compact[max_idx]:.4f}, f(x) = {y_compact[max_idx]:.4f}\")\n",
    "\n",
    "# Cas 2: Non-compact\n",
    "x_non_compact = np.linspace(0, 10, 1000)\n",
    "y_non_compact = f_non_compact(x_non_compact)\n",
    "\n",
    "print(\"\\n=== Exemple 2 : Fonction sur [0, +∞) (NON compact) ===\")\n",
    "print(f\"f(x) = e^(-x) sur [0, +∞)\")\n",
    "print(f\"\\nMaximum ATTEINT en x = 0, f(0) = {f_non_compact(0):.4f}\")\n",
    "print(f\"Infimum = 0 mais JAMAIS ATTEINT (pas de x tel que e^(-x) = 0)\")\n",
    "print(f\"f(10) = {f_non_compact(10):.6f} (proche de 0 mais ≠ 0)\")\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Compact\n",
    "ax1.plot(x_compact, y_compact, 'b-', linewidth=2.5)\n",
    "ax1.scatter([x_compact[min_idx]], [y_compact[min_idx]], \n",
    "           c='red', s=300, zorder=5, marker='v', edgecolors='black', linewidth=2,\n",
    "           label=f'Minimum ATTEINT\\nx={x_compact[min_idx]:.3f}')\n",
    "ax1.scatter([x_compact[max_idx]], [y_compact[max_idx]], \n",
    "           c='green', s=300, zorder=5, marker='^', edgecolors='black', linewidth=2,\n",
    "           label=f'Maximum ATTEINT\\nx={x_compact[max_idx]:.3f}')\n",
    "ax1.axvline(x=0, color='purple', linestyle='--', alpha=0.5, linewidth=2)\n",
    "ax1.axvline(x=2*np.pi, color='purple', linestyle='--', alpha=0.5, linewidth=2)\n",
    "ax1.fill_between([0, 2*np.pi], -2, 2, alpha=0.1, color='purple', \n",
    "                label='Compact K = [0, 2π]')\n",
    "ax1.set_xlabel('x', fontweight='bold', fontsize=12)\n",
    "ax1.set_ylabel('f(x)', fontweight='bold', fontsize=12)\n",
    "ax1.set_title('Sur un COMPACT\\nles bornes sont ATTEINTES', \n",
    "             fontweight='bold', fontsize=14, color='green')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Non-compact\n",
    "ax2.plot(x_non_compact, y_non_compact, 'r-', linewidth=2.5)\n",
    "ax2.scatter([0], [f_non_compact(0)], c='green', s=300, zorder=5, \n",
    "           marker='^', edgecolors='black', linewidth=2,\n",
    "           label='Maximum ATTEINT\\nx=0, f(0)=1')\n",
    "ax2.axhline(y=0, color='blue', linestyle='--', linewidth=2, \n",
    "           label='Infimum = 0\\n(NON ATTEINT!)')\n",
    "ax2.annotate('Tend vers 0\\nmais ne l\\'atteint jamais', \n",
    "            xy=(7, f_non_compact(7)), xytext=(5, 0.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            fontsize=11, color='red',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "ax2.set_xlabel('x', fontweight='bold', fontsize=12)\n",
    "ax2.set_ylabel('f(x) = e^(-x)', fontweight='bold', fontsize=12)\n",
    "ax2.set_title('Sur [0,+∞) (NON COMPACT)\\ninfimum NON ATTEINT', \n",
    "             fontweight='bold', fontsize=14, color='red')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([-0.1, 1.2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/user-data/outputs/weierstrass.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Application 1.12 : Optimisation de Portefeuille (Markowitz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Application 1.12 : Optimisation de Portefeuille (Markowitz) ===\")\n",
    "print(\"\\nProblème :\")\n",
    "print(\"  min w^T Σ w\")\n",
    "print(\"  s.t. Σwᵢ = 1, wᵢ ≥ 0\")\n",
    "print(\"\\nAnalyse :\")\n",
    "print(\"  • L'ensemble des contraintes est COMPACT\")\n",
    "print(\"    (fermé et borné dans ℝⁿ : c'est un simplexe)\")\n",
    "print(\"  • La fonction objectif w ↦ w^T Σ w est CONTINUE\")\n",
    "print(\"  • Par le théorème de Weierstrass : le minimum EXISTE\")\n",
    "\n",
    "# Exemple avec 3 actifs\n",
    "np.random.seed(42)\n",
    "n_assets = 3\n",
    "\n",
    "# Matrice de covariance (définie positive)\n",
    "A = np.random.randn(n_assets, n_assets)\n",
    "Sigma = A.T @ A / 10 + np.eye(n_assets) * 0.1\n",
    "\n",
    "print(f\"\\n=== Exemple avec {n_assets} actifs ===\")\n",
    "print(f\"\\nMatrice de covariance Σ :\")\n",
    "print(Sigma)\n",
    "\n",
    "# Définir la fonction objectif\n",
    "def risque_portefeuille(w):\n",
    "    return w @ Sigma @ w\n",
    "\n",
    "# Contraintes\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "constraints = (\n",
    "    {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},  # Σwᵢ = 1\n",
    ")\n",
    "bounds = [(0, 1) for _ in range(n_assets)]  # wᵢ ≥ 0\n",
    "\n",
    "# Point initial\n",
    "w0 = np.ones(n_assets) / n_assets  # Équipondéré\n",
    "\n",
    "# Optimisation\n",
    "result = minimize(risque_portefeuille, w0, method='SLSQP', \n",
    "                 bounds=bounds, constraints=constraints)\n",
    "\n",
    "w_optimal = result.x\n",
    "risque_optimal = result.fun\n",
    "\n",
    "print(f\"\\nPortefeuille équipondéré (point initial) :\")\n",
    "print(f\"  w₀ = {w0}\")\n",
    "print(f\"  Risque = {risque_portefeuille(w0):.6f}\")\n",
    "\n",
    "print(f\"\\nPortefeuille optimal (minimum de variance) :\")\n",
    "print(f\"  w* = {w_optimal}\")\n",
    "print(f\"  Risque = {risque_optimal:.6f}\")\n",
    "print(f\"  Réduction du risque : {(1 - risque_optimal/risque_portefeuille(w0))*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nVérification des contraintes :\")\n",
    "print(f\"  Σwᵢ = {np.sum(w_optimal):.10f} (doit être = 1)\")\n",
    "print(f\"  min(wᵢ) = {np.min(w_optimal):.6f} (doit être ≥ 0)\")\n",
    "print(f\"\\n✓ Le minimum EXISTE et EST ATTEINT (grâce au théorème de Weierstrass)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du portefeuille optimal\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Graphique 1: Comparaison des portefeuilles\n",
    "ax1 = fig.add_subplot(121)\n",
    "x = np.arange(n_assets)\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, w0, width, label='Équipondéré', alpha=0.8, edgecolor='black')\n",
    "ax1.bar(x + width/2, w_optimal, width, label='Optimal (min variance)', \n",
    "       alpha=0.8, color='green', edgecolor='black')\n",
    "ax1.set_xlabel('Actif', fontweight='bold', fontsize=12)\n",
    "ax1.set_ylabel('Poids', fontweight='bold', fontsize=12)\n",
    "ax1.set_title('Portefeuilles\\nÉquipondéré vs Optimal', fontweight='bold', fontsize=14)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f'Actif {i+1}' for i in range(n_assets)])\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Graphique 2: Frontière efficiente (pour 2 actifs)\n",
    "if n_assets >= 2:\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    # Explorer différentes allocations\n",
    "    n_points = 100\n",
    "    w1_range = np.linspace(0, 1, n_points)\n",
    "    risques = []\n",
    "    \n",
    "    for w1 in w1_range:\n",
    "        if n_assets == 2:\n",
    "            w = np.array([w1, 1-w1])\n",
    "        else:\n",
    "            # Pour 3 actifs, on fixe w1 et optimise sur w2, w3\n",
    "            w_temp = np.array([w1, (1-w1)/2, (1-w1)/2])\n",
    "            w = w_temp\n",
    "        \n",
    "        if np.all(w >= 0) and np.abs(np.sum(w) - 1) < 1e-6:\n",
    "            risques.append(risque_portefeuille(w))\n",
    "        else:\n",
    "            risques.append(np.nan)\n",
    "    \n",
    "    ax2.plot(w1_range, risques, 'b-', linewidth=2, label='Risque du portefeuille')\n",
    "    ax2.scatter([w_optimal[0]], [risque_optimal], c='red', s=300, \n",
    "               marker='*', edgecolors='black', linewidth=2, zorder=5,\n",
    "               label=f'Optimal: w₁={w_optimal[0]:.3f}')\n",
    "    ax2.scatter([w0[0]], [risque_portefeuille(w0)], c='green', s=200, \n",
    "               marker='o', edgecolors='black', linewidth=2, zorder=5,\n",
    "               label=f'Équipondéré: w₁={w0[0]:.3f}')\n",
    "    \n",
    "    ax2.set_xlabel('Poids du premier actif (w₁)', fontweight='bold', fontsize=12)\n",
    "    ax2.set_ylabel('Risque (w^T Σ w)', fontweight='bold', fontsize=12)\n",
    "    ax2.set_title('Fonction de Risque\\nMinimum ATTEINT', fontweight='bold', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/user-data/outputs/markowitz.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Synthèse Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SYNTHÈSE : APPLICATIONS ML ET CONTINUITÉ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARTIE 1 : APPLICATIONS AU MACHINE LEARNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. ESPACES DE PARAMÈTRES\")\n",
    "print(\"   • Modèle ML → paramètres θ ∈ ℝᵐ\")\n",
    "print(\"   • Exemples : coefficients régression, poids réseaux neurones\")\n",
    "\n",
    "print(\"\\n2. CONVERGENCE DES ALGORITHMES\")\n",
    "print(\"   • Descente de gradient : θ^(k+1) = θ^(k) - α∇f(θ^(k))\")\n",
    "print(\"   • Théorème : Si f convexe, gradients Lipschitz, pas bien choisi\")\n",
    "print(\"                → θ^(k) → θ* (minimum global)\")\n",
    "print(\"   • Critères pratiques :\")\n",
    "print(\"     - ||θ^(k+1) - θ^(k)|| < ε (variation paramètres)\")\n",
    "print(\"     - ||∇f(θ^(k))|| < ε (gradient proche de 0)\")\n",
    "\n",
    "print(\"\\n3. RÉGULARISATION\")\n",
    "print(\"   • Problème : min_θ [L(θ) + λR(θ)]\")\n",
    "print(\"   • L2 (Ridge) : R(θ) = ||θ||₂²\")\n",
    "print(\"     → Paramètres petits, solution stable\")\n",
    "print(\"   • L1 (LASSO) : R(θ) = ||θ||₁\")\n",
    "print(\"     → PARCIMONIE, sélection automatique de features\")\n",
    "print(\"   • Application finance : Sélectionner 5-10 indicateurs sur 100\")\n",
    "\n",
    "print(\"\\n4. DISTANCES ET SIMILARITÉ\")\n",
    "print(\"   • Distance entre modèles : d(θ₁, θ₂) = ||θ₁ - θ₂||₂\")\n",
    "print(\"   • Distance entre prédictions : Erreur = ||y - ŷ||₂\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARTIE 2 : CONTINUITÉ ET THÉORÈMES FONDAMENTAUX\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n5. DÉFINITION DE CONTINUITÉ\")\n",
    "print(\"   • f continue en x₀ si :\")\n",
    "print(\"     ∀ε > 0, ∃δ > 0 : |x-x₀| < δ ⟹ |f(x)-f(x₀)| < ε\")\n",
    "print(\"   • Équivalent : xₙ → x₀ ⟹ f(xₙ) → f(x₀)\")\n",
    "print(\"   • Interprétation : Petites variations entrée → petites variations sortie\")\n",
    "\n",
    "print(\"\\n6. FONCTIONS DE PERTE (ML)\")\n",
    "print(\"   • MSE : L(θ) = (1/n)Σ(y - f_θ(x))² → CONTINUE\")\n",
    "print(\"   • Cross-entropy : L(θ) = -Σy log(p_θ(x)) → CONTINUE\")\n",
    "print(\"   • Importance : Continuité permet optimisation par gradient\")\n",
    "\n",
    "print(\"\\n7. THÉORÈME DES VALEURS INTERMÉDIAIRES (TVI)\")\n",
    "print(\"   • Si f : [a,b] → ℝ continue et f(a)·f(b) < 0\")\n",
    "print(\"     → ∃c : f(c) = 0\")\n",
    "print(\"   • Application : Algorithme de dichotomie\")\n",
    "print(\"     - Convergence en O(log(1/ε)) itérations\")\n",
    "print(\"     - Trouve zéros, points fixes, équilibres\")\n",
    "\n",
    "print(\"\\n8. THÉORÈME DES BORNES ATTEINTES (Weierstrass)\")\n",
    "print(\"   • Si f continue sur COMPACT K\")\n",
    "print(\"     → f est bornée et ATTEINT ses bornes\")\n",
    "print(\"   • CRUCIAL : Sur compact, min f(x) EXISTE TOUJOURS\")\n",
    "print(\"   • Application : Optimisation de portefeuille Markowitz\")\n",
    "print(\"     - Contraintes = compact (fermé et borné)\")\n",
    "print(\"     - Fonction continue → minimum existe\")\n",
    "\n",
    "print(\"\\n9. IMPORTANCE EN ML/FINANCE\")\n",
    "print(\"   • Continuité des fonctions de perte → optimisation possible\")\n",
    "print(\"   • Compacité → existence de solutions optimales\")\n",
    "print(\"   • TVI → algorithmes de recherche de zéros/équilibres\")\n",
    "print(\"   • Weierstrass → garantie d'existence de portefeuille optimal\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Fichiers générés :\")\n",
    "print(\"  • convergence_ml.png\")\n",
    "print(\"  • regularisation_ml.png\")\n",
    "print(\"  • distances_modeles.png\")\n",
    "print(\"  • continuite_fonctions.png\")\n",
    "print(\"  • fonctions_perte.png\")\n",
    "print(\"  • dichotomie.png\")\n",
    "print(\"  • weierstrass.png\")\n",
    "print(\"  • markowitz.png\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
