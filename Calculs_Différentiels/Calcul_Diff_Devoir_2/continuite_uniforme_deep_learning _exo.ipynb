{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuité Uniforme et Applications au Deep Learning\n",
    "## Illustrations et Applications Pratiques\n",
    "\n",
    "Ce notebook illustre les concepts de continuité uniforme, fonctions lipschitziennes, et leurs applications au machine learning (fonctions de perte et d'activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from scipy.special import expit  # sigmoid\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration pour de meilleurs graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Continuité Uniforme vs Continuité Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Continuité Uniforme ===\n",
      "\n",
      "Définition :\n",
      "  f est UNIFORMÉMENT continue si :\n",
      "  ∀ε > 0, ∃δ > 0, ∀x,y ∈ X : |x-y| < δ ⟹ |f(x)-f(y)| < ε\n",
      "\n",
      "Différence avec la continuité simple :\n",
      "  • Continuité simple : δ dépend de ε ET du point x₀\n",
      "  • Continuité UNIFORME : δ dépend UNIQUEMENT de ε (même δ pour tous les points)\n",
      "\n",
      "=== Exemple 1.14 : f(x) = x² sur ℝ ===\n",
      "\n",
      "f est CONTINUE partout, mais PAS uniformément continue\n",
      "\n",
      "Preuve : Pour x grand, la pente f'(x) = 2x devient arbitrairement grande\n",
      "On peut toujours trouver x et y proches tels que |f(x) - f(y)| > ε\n",
      "\n",
      "Test avec ε = 1.0, δ = 0.1 :\n",
      "\n",
      "       x      y=x+δ     |f(x)-f(y)|      < ε ?\n",
      "--------------------------------------------------\n",
      "       1        1.1            0.21          ✓\n",
      "      10       10.1            2.01          ✗\n",
      "     100      100.1           20.01          ✗\n",
      "    1000     1000.1          200.01          ✗\n",
      "\n",
      "Conclusion : Pour x grand, même avec δ = 0.1, |f(x)-f(y)| > 1.0\n",
      "Donc f(x) = x² n'est PAS uniformément continue sur ℝ\n",
      "\n",
      "=== Exemple 1.15 : f(x) = x² sur [0,1] ===\n",
      "\n",
      "Sur un intervalle COMPACT [0,1], f est uniformément continue\n",
      "(Théorème de Heine)\n",
      "\n",
      "Pour ε > 0, on peut choisir δ = ε/2 :\n",
      "Si |x - y| < δ, alors |x² - y²| = |x+y||x-y| ≤ 2|x-y| < 2δ = ε\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Continuité Uniforme ===\")\n",
    "print(\"\\nDéfinition :\")\n",
    "print(\"  f est UNIFORMÉMENT continue si :\")\n",
    "print(\"  ∀ε > 0, ∃δ > 0, ∀x,y ∈ X : |x-y| < δ ⟹ |f(x)-f(y)| < ε\")\n",
    "print(\"\\nDifférence avec la continuité simple :\")\n",
    "print(\"  • Continuité simple : δ dépend de ε ET du point x₀\")\n",
    "print(\"  • Continuité UNIFORME : δ dépend UNIQUEMENT de ε (même δ pour tous les points)\")\n",
    "\n",
    "# Exemple 1.14 : Continue mais PAS uniformément continue\n",
    "def f_non_uniforme(x):\n",
    "    \"\"\"f(x) = x² sur ℝ : continue mais pas uniformément continue\"\"\"\n",
    "    return x**2\n",
    "\n",
    "# Exemple 1.15 : Uniformément continue\n",
    "def f_uniforme(x):\n",
    "    \"\"\"f(x) = x² sur [0,1] : uniformément continue (compact)\"\"\"\n",
    "    return x**2\n",
    "\n",
    "print(\"\\n=== Exemple 1.14 : f(x) = x² sur ℝ ===\")\n",
    "print(\"\\nf est CONTINUE partout, mais PAS uniformément continue\")\n",
    "print(\"\\nPreuve : Pour x grand, la pente f'(x) = 2x devient arbitrairement grande\")\n",
    "print(\"On peut toujours trouver x et y proches tels que |f(x) - f(y)| > ε\")\n",
    "\n",
    "# Démonstration numérique\n",
    "epsilon = 1.0\n",
    "x_values = [1, 10, 100, 1000]\n",
    "delta = 0.1\n",
    "\n",
    "print(f\"\\nTest avec ε = {epsilon}, δ = {delta} :\")\n",
    "print(f\"\\n{'x':>8} {'y=x+δ':>10} {'|f(x)-f(y)|':>15} {'< ε ?':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for x in x_values:\n",
    "    y = x + delta\n",
    "    diff = abs(f_non_uniforme(x) - f_non_uniforme(y))\n",
    "    check = \"✓\" if diff < epsilon else \"✗\"\n",
    "    print(f\"{x:8.0f} {y:10.1f} {diff:15.2f} {check:>10}\")\n",
    "\n",
    "print(f\"\\nConclusion : Pour x grand, même avec δ = {delta}, |f(x)-f(y)| > {epsilon}\")\n",
    "print(\"Donc f(x) = x² n'est PAS uniformément continue sur ℝ\")\n",
    "\n",
    "print(\"\\n=== Exemple 1.15 : f(x) = x² sur [0,1] ===\")\n",
    "print(\"\\nSur un intervalle COMPACT [0,1], f est uniformément continue\")\n",
    "print(\"(Théorème de Heine)\")\n",
    "\n",
    "# Sur [0,1], la dérivée max est f'(1) = 2\n",
    "print(\"\\nPour ε > 0, on peut choisir δ = ε/2 :\")\n",
    "print(\"Si |x - y| < δ, alors |x² - y²| = |x+y||x-y| ≤ 2|x-y| < 2δ = ε\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la continuité uniforme",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))",
    "",
    "# Graphique 1: f(x) = x² sur ℝ (zoom sur différentes régions)",
    "x1 = np.linspace(0, 2, 100)",
    "x2 = np.linspace(8, 10, 100)",
    "",
    "# TODO: Définir f_non_uniforme(x) = x²",
    "f_non_uniforme = lambda x: ...",
    "",
    "# TODO: Tracer la fonction sur les deux régions",
    "# ax1.plot(x1, f_non_uniforme(x1), ...)",
    "# ax1.plot(x2, f_non_uniforme(x2), ...)",
    "",
    "plt.tight_layout()",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Continuité Lipschitzienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Continuité Lipschitzienne ===\n",
      "\n",
      "Définition :\n",
      "  f est L-Lipschitzienne si :\n",
      "  |f(x) - f(y)| ≤ L·|x - y| pour tous x, y\n",
      "  |f(x) - f(y)| /|x-y| ≤ L pour tous x, y\n",
      "\n",
      "Hiérarchie :\n",
      "  Lipschitz ⟹ Uniformément continue ⟹ Continue\n",
      "  (Les réciproques sont FAUSSES en général)\n",
      "\n",
      "=== Exemple 1.16 : Exemples de fonctions ===\n",
      "\n",
      "1. f(x) = x (identité)\n",
      "   |f(x) - f(y)| = |x - y|\n",
      "   → 1-Lipschitz (L = 1)\n",
      "\n",
      "2. f(x) = sin(x)\n",
      "   |sin(x) - sin(y)| ≤ |x - y| (car |cos(z)| ≤ 1)\n",
      "   → 1-Lipschitz (L = 1)\n",
      "\n",
      "3. f(x) = √x sur [0, ∞)\n",
      "   PAS Lipschitz (dérivée infinie en 0)\n",
      "   f'(x) = 1/(2√x) → ∞ quand x → 0⁺\n",
      "\n",
      "4. f(x) = x² sur [0,1]\n",
      "   |f'(x)| = |2x| ≤ 2 sur [0,1]\n",
      "   → 2-Lipschitz (L = 2)\n",
      "\n",
      "=== Vérification Empirique ===\n",
      "\n",
      "Constante de Lipschitz empirique :\n",
      "  • f(x) = x      sur [0,10]  : L ≈ 1.0000\n",
      "  • f(x) = sin(x) sur [0,2π]  : L ≈ 1.0000\n",
      "  • f(x) = x²     sur [0,1]   : L ≈ 1.9990\n",
      "  • f(x) = √x     sur [0.01,1]: L ≈ 4.8819\n",
      "  • f(x) = √x     sur [0,1]   : L ≈ 30.6 (tend vers ∞!)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Continuité Lipschitzienne ===\")\n",
    "print(\"\\nDéfinition :\")\n",
    "print(\"  f est L-Lipschitzienne si :\")\n",
    "print(\"  |f(x) - f(y)| ≤ L·|x - y| pour tous x, y\")\n",
    "print(\"  |f(x) - f(y)| /|x-y| ≤ L pour tous x, y\")\n",
    "print(\"\\nHiérarchie :\")\n",
    "print(\"  Lipschitz ⟹ Uniformément continue ⟹ Continue\")\n",
    "print(\"  (Les réciproques sont FAUSSES en général)\")\n",
    "\n",
    "# Fonctions d'exemple\n",
    "def identite(x):\n",
    "    return x\n",
    "\n",
    "def sinus(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def racine(x):\n",
    "    return np.sqrt(np.maximum(x, 0))\n",
    "\n",
    "def carre_compact(x):\n",
    "    return x**2\n",
    "\n",
    "print(\"\\n=== Exemple 1.16 : Exemples de fonctions ===\")\n",
    "print(\"\\n1. f(x) = x (identité)\")\n",
    "print(\"   |f(x) - f(y)| = |x - y|\")\n",
    "print(\"   → 1-Lipschitz (L = 1)\")\n",
    "\n",
    "print(\"\\n2. f(x) = sin(x)\")\n",
    "print(\"   |sin(x) - sin(y)| ≤ |x - y| (car |cos(z)| ≤ 1)\")\n",
    "print(\"   → 1-Lipschitz (L = 1)\")\n",
    "\n",
    "print(\"\\n3. f(x) = √x sur [0, ∞)\")\n",
    "print(\"   PAS Lipschitz (dérivée infinie en 0)\")\n",
    "print(\"   f'(x) = 1/(2√x) → ∞ quand x → 0⁺\")\n",
    "\n",
    "print(\"\\n4. f(x) = x² sur [0,1]\")\n",
    "print(\"   |f'(x)| = |2x| ≤ 2 sur [0,1]\")\n",
    "print(\"   → 2-Lipschitz (L = 2)\")\n",
    "\n",
    "# Vérification numérique de la constante de Lipschitz\n",
    "def constante_lipschitz_empirique(f, x_min, x_max, n_points=1000):\n",
    "    \"\"\"Estime la constante de Lipschitz empiriquement\"\"\"\n",
    "    x = np.linspace(x_min, x_max, n_points)\n",
    "    y = f(x)\n",
    "    \n",
    "    # Calculer toutes les pentes\n",
    "    ratios = []\n",
    "    for i in range(len(x)):\n",
    "        for j in range(i+1, min(i+50, len(x))):  # Limiter pour la performance\n",
    "            if x[j] != x[i]:\n",
    "                ratio = abs(y[j] - y[i]) / abs(x[j] - x[i])\n",
    "                ratios.append(ratio)\n",
    "    \n",
    "    return max(ratios) if ratios else 0\n",
    "\n",
    "print(\"\\n=== Vérification Empirique ===\")\n",
    "print(f\"\\nConstante de Lipschitz empirique :\")\n",
    "print(f\"  • f(x) = x      sur [0,10]  : L ≈ {constante_lipschitz_empirique(identite, 0, 10):.4f}\")\n",
    "print(f\"  • f(x) = sin(x) sur [0,2π]  : L ≈ {constante_lipschitz_empirique(sinus, 0, 2*np.pi):.4f}\")\n",
    "print(f\"  • f(x) = x²     sur [0,1]   : L ≈ {constante_lipschitz_empirique(carre_compact, 0, 1):.4f}\")\n",
    "print(f\"  • f(x) = √x     sur [0.01,1]: L ≈ {constante_lipschitz_empirique(racine, 0.01, 1):.4f}\")\n",
    "print(f\"  • f(x) = √x     sur [0,1]   : L ≈ {constante_lipschitz_empirique(racine, 1e-6, 1):.1f} (tend vers ∞!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des fonctions lipschitziennes",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))",
    "",
    "# f(x) = x (1-Lipschitz)",
    "x = np.linspace(-2, 2, 100)",
    "# TODO: Définir identite(x) = x",
    "identite = lambda x: ...",
    "",
    "# TODO: Tracer la fonction identité",
    "# axes[0, 0].plot(x, identite(x), ...)",
    "",
    "# TODO: Tracer les cônes de Lipschitz pour différentes valeurs de L",
    "x0, y0 = 0, 0",
    "for L in [0.5, 1, 2]:",
    "    # axes[0, 0].plot(x, y0 + L*(x - x0), ...)",
    "    pass",
    "",
    "plt.tight_layout()",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Application 1.15 : Convergence de la Descente de Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Application 1.15 : Convergence de la Descente de Gradient ===\")",
    "print(\"\\nThéorème (simplifié) :\")",
    "print(\"  Si f est convexe et ∇f est L-Lipschitz,\")",
    "print(\"  alors la descente de gradient avec α = 1/L\")",
    "print(\"  converge vers le minimum à un taux O(1/k)\")",
    "",
    "# TODO: Implémenter la descente de gradient",
    "# def gradient_descent(f, grad_f, x0, alpha, n_iter):",
    "#     ...",
    "",
    "print(\"\\nLa constante L contrôle la vitesse de convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))",
    "",
    "# Graphique 1: Trajectoires",
    "# TODO: Tracer les trajectoires de convergence pour différents learning rates",
    "# for name, hist in historiques.items():",
    "#     ax1.plot(range(len(hist)), hist, ...)",
    "",
    "# Graphique 2: Comparaison des vitesses",
    "# TODO: Comparer les vitesses de convergence",
    "",
    "plt.tight_layout()",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonctions de Perte en Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Fonctions de Perte en Machine Learning ===\")\n",
    "print(\"\\nLa fonction de perte L(θ) mesure l'erreur du modèle\")\n",
    "print(\"\\nPropriétés souhaitables :\")\n",
    "print(\"  • CONTINUITÉ : petites variations θ → petites variations L\")\n",
    "print(\"  • DIFFÉRENTIABILITÉ : pour optimiser par gradient\")\n",
    "print(\"  • CONVEXITÉ : garantit unique minimum global\")\n",
    "\n",
    "# Définir les fonctions de perte\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"Mean Squared Error\"\"\"\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def cross_entropy_loss(y_true, p_pred):\n",
    "    \"\"\"Binary Cross-Entropy\"\"\"\n",
    "    epsilon = 1e-15\n",
    "    p_pred = np.clip(p_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(p_pred) + (1 - y_true) * np.log(1 - p_pred))\n",
    "\n",
    "def hinge_loss(y_true, y_pred):\n",
    "    \"\"\"Hinge Loss (SVM) - y_true ∈ {-1, 1}\"\"\"\n",
    "    return np.mean(np.maximum(0, 1 - y_true * y_pred))\n",
    "\n",
    "def mae_loss(y_true, y_pred):\n",
    "    \"\"\"Mean Absolute Error\"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "print(\"\\n=== Exemple 1.17 : Mean Squared Error (MSE) ===\")\n",
    "print(\"\\nL(θ) = (1/n)Σ(yᵢ - f_θ(xᵢ))²\")\n",
    "print(\"\\nPropriétés :\")\n",
    "print(\"  ✓ Continue si f_θ continue en θ\")\n",
    "print(\"  ✓ Différentiable si f_θ différentiable\")\n",
    "print(\"  ✓ Convexe si f_θ affine en θ (régression linéaire)\")\n",
    "\n",
    "print(\"\\n=== Exemple 1.18 : Cross-Entropy ===\")\n",
    "print(\"\\nL(θ) = -(1/n)Σ[yᵢ log(p_θ(xᵢ)) + (1-yᵢ)log(1-p_θ(xᵢ))]\")\n",
    "print(\"\\nPropriétés :\")\n",
    "print(\"  ✓ Continue pour p ∈ (0,1)\")\n",
    "print(\"  ✓ Différentiable\")\n",
    "print(\"  ✓ Convexe pour régression logistique\")\n",
    "\n",
    "print(\"\\n=== Exemple 1.19 : Hinge Loss (SVM) ===\")\n",
    "print(\"\\nL(θ) = (1/n)Σ max(0, 1 - yᵢ·θᵀxᵢ)\")\n",
    "print(\"\\nPropriétés :\")\n",
    "print(\"  ✓ Continue partout\")\n",
    "print(\"  ✗ Non différentiable en θᵀxᵢ = yᵢ (mais sous-différentiable)\")\n",
    "print(\"  ✓ Convexe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))",
    "",
    "# Graphique 1: Trajectoires",
    "# TODO: Tracer les trajectoires de convergence pour différents learning rates",
    "# for name, hist in historiques.items():",
    "#     ax1.plot(range(len(hist)), hist, ...)",
    "",
    "# Graphique 2: Comparaison des vitesses",
    "# TODO: Comparer les vitesses de convergence",
    "",
    "plt.tight_layout()",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fonctions d'Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Fonctions d'Activation ===\")",
    "print(\"\\nDans les réseaux de neurones, chaque neurone applique\")",
    "print(\"une fonction d'activation NON-LINÉAIRE\")",
    "",
    "# Définir les fonctions d'activation",
    "def sigmoid(z):",
    "    \"\"\"Sigmoïde : σ(z) = 1/(1 + e^(-z))\"\"\"",
    "    # TODO: Implémenter la fonction sigmoïde",
    "    return ...",
    "",
    "def sigmoid_derivative(z):",
    "    \"\"\"Dérivée : σ'(z) = σ(z)(1 - σ(z))\"\"\"",
    "    # TODO: Implémenter la dérivée",
    "    return ...",
    "",
    "def relu(z):",
    "    \"\"\"ReLU : max(0, z)\"\"\"",
    "    # TODO: Implémenter ReLU",
    "    return ...",
    "",
    "def tanh(z):",
    "    \"\"\"Tangente hyperbolique\"\"\"",
    "    # TODO: Implémenter tanh",
    "    return ...",
    "",
    "print(\"Fonctions d'activation définies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))",
    "",
    "# Graphique 1: Trajectoires",
    "# TODO: Tracer les trajectoires de convergence pour différents learning rates",
    "# for name, hist in historiques.items():",
    "#     ax1.plot(range(len(hist)), hist, ...)",
    "",
    "# Graphique 2: Comparaison des vitesses",
    "# TODO: Comparer les vitesses de convergence",
    "",
    "plt.tight_layout()",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Théorème d'Approximation Universelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Théorème d'Approximation Universelle ===\")",
    "print(\"\\nThéorème (Cybenko, 1989) :\")",
    "print(\"  Un réseau de neurones à UNE couche cachée avec fonction\")",
    "print(\"  d'activation sigmoïde peut approximer uniformément TOUTE\")",
    "print(\"  fonction continue sur un compact, avec précision arbitraire\")",
    "",
    "# TODO: Créer un réseau simple pour approximer une fonction",
    "# def simple_network(x, weights, biases):",
    "#     ...",
    "",
    "print(\"\\nCe théorème justifie l'utilisation des réseaux de neurones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))",
    "",
    "# Graphique 1: Trajectoires",
    "# TODO: Tracer les trajectoires de convergence pour différents learning rates",
    "# for name, hist in historiques.items():",
    "#     ax1.plot(range(len(hist)), hist, ...)",
    "",
    "# Graphique 2: Comparaison des vitesses",
    "# TODO: Comparer les vitesses de convergence",
    "",
    "plt.tight_layout()",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Convexité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Convexité ===\")",
    "print(\"\\nDéfinition :\")",
    "print(\"  f est convexe si pour tous x, y et λ ∈ [0,1] :\")",
    "print(\"  f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)\")",
    "",
    "# TODO: Définir une fonction convexe",
    "def f_convexe(x):",
    "    # TODO: Implémenter f(x) = x²",
    "    return ...",
    "",
    "# TODO: Définir une fonction non-convexe",
    "def f_non_convexe(x):",
    "    # TODO: Implémenter f(x) = -x² ou sin(x)",
    "    return ...",
    "",
    "print(\"\\nPropriété : toute fonction convexe a un unique minimum global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))",
    "",
    "# Graphique 1: Trajectoires",
    "# TODO: Tracer les trajectoires de convergence pour différents learning rates",
    "# for name, hist in historiques.items():",
    "#     ax1.plot(range(len(hist)), hist, ...)",
    "",
    "# Graphique 2: Comparaison des vitesses",
    "# TODO: Comparer les vitesses de convergence",
    "",
    "plt.tight_layout()",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Synthèse Finale - Module Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)",
    "print(\"SYNTHÈSE COMPLÈTE : MODULE 1 - ANALYSE MATHÉMATIQUE POUR ML\")",
    "print(\"=\"*80)",
    "",
    "# TODO: Résumer les concepts clés du module",
    "# - Espaces métriques",
    "# - Continuité uniforme et Lipschitz",
    "# - Fonctions de perte et d'activation",
    "# - Théorème d'approximation universelle",
    "# - Convexité et optimisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}